{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chart Events for last 48 period"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "\n",
    "1. [ ] Create or find list of `ITEMID`s that correlate to the 17 dimmensions from paper.\n",
    "2. [ ] Extract the 17 dims from `CHARTEVENTS.csv` for each sample"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|\tChart Event\t|\tDim\t|\tNormal\t|\n",
    "| --- | :--- | :--- |\n",
    "|\t1. Glasgow coma scale eye opening\t|\t4\t|\t4 Spontaneously\t|\n",
    "|\t2. Glasgow coma scale verbal response\t|\t5\t|\t5 Oriented\t|\n",
    "|\t3. Glasgow coma scale motor response\t|\t6\t|\t6 Obeys Commands\t|\n",
    "|\t4. Glasgow coma scale total\t|\t13\t|\t15\t|\n",
    "|\t5. Capillary refill rate\t|\t2\t|\tNormal < 3 secs\t|\n",
    "|\t6. Diastolic blood pressure\t|\t1\t|\t70\t|\n",
    "|\t7. Systolic blood pressure\t|\t1\t|\t105\t|\n",
    "|\t8. Mean blood pressure\t|\t1\t|\t87.5\t|\n",
    "|\t9. Heart Rate\t|\t1\t|\t80\t|\n",
    "|\t10. Glucose\t|\t1\t|\t85\t|\n",
    "|\t11. Fraction inspired oxygen\t|\t1\t|\t0.21\t|\n",
    "|\t12. Oxygen saturation\t|\t1\t|\t97.5\t|\n",
    "|\t13. Respiratory rate\t|\t1\t|\t15\t|\n",
    "|\t14. Body Temperature\t|\t1\t|\t37\t|\n",
    "|\t15. pH\t|\t1\t|\t7.4\t|\n",
    "|\t16. Weight\t|\t1\t|\t80.7\t|\n",
    "|\t17. Height\t|\t1\t|\t168.8\t|\n",
    "\n",
    "![Glascow](assets/images/GCS.jpg)\n",
    "\n",
    "https://www.firstaidforfree.com/glasgow-coma-scale-gcs-first-aiders/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_to_id =[\n",
    "{'CHAREVENT':'GCS_EYE', 'DESCRIPTION':'','ITEMID':['184','220739'],'UNIT':['NONE','NONE']},\n",
    "{'CHAREVENT':'GCS_MOTOR', 'DESCRIPTION':'','ITEMID':['454','223901'],'UNIT':['NONE','NONE']},\n",
    "{'CHAREVENT':'GCS_VERBAL', 'DESCRIPTION':'','ITEMID':['723','223900'],'UNIT':['NONE','NONE']},\n",
    "{'CHAREVENT':'GCS_TOTAL', 'DESCRIPTION':'Sum of the 3 GCS events','ITEMID':['198'],'UNIT':['NONE']},\n",
    "{'CHAREVENT':'CAPILLARY_REFILL', 'DESCRIPTION':'','ITEMID':['3348','115','8377','223951','224308'],'UNIT':['BINARY','BINARY','BINARY','BINARY','BINARY']},\n",
    "{'CHAREVENT':'D_BLOOD_PRESSURE', 'DESCRIPTION':'','ITEMID':['8368','220051','225310','8555','8441','220180','8502','8440','8503','8504','8507','8506','224643','227242'],'UNIT':['mmHg','mmHg','mmHg','mmHg','mmHg','mmHg','mmHg','mmHg','mmHg','mmHg','mmHg','mmHg','mmHg','mmHg']},\n",
    "{'CHAREVENT':'M_BLOOD_PRESSURE', 'DESCRIPTION':'','ITEMID':['52', '220052', '225312', '224', '6702', '224322', '456', '220181', '3312', '3314', '3316', '3322', '3320'],'UNIT':['mmHg','mmHg','mmHg','mmHg','mmHg','mmHg','mmHg','mmHg','mmHg','mmHg','mmHg','mmHg','mmHg']},\n",
    "{'CHAREVENT':'S_BLOOD_PRESSURE', 'DESCRIPTION':'','ITEMID':['51','220050','225309','6701','455','220179','3313','3315','442','3317','3323','3321','224167','227243'],'UNIT':['mmHg','mmHg','mmHg','mmHg','mmHg','mmHg','mmHg','mmHg','mmHg','mmHg','mmHg','mmHg','mmHg','mmHg']},\n",
    "{'CHAREVENT':'HEART_RATE', 'DESCRIPTION':'','ITEMID':['211','220045'],'UNIT':['bpm','bpm']},\n",
    "{'CHAREVENT':'GLUCOSE', 'DESCRIPTION':'','ITEMID':['807', '811', '1529', '3745', '225664', '220621', '226537', '3447', '3816', '3744', '227015', '227016', '1310', '1455', '2338', '1812', '228388'],'UNIT':['','']},\n",
    "{'CHAREVENT':'FRAC_OXYGEN', 'DESCRIPTION':'','ITEMID':['3420', '223835', '3422', '189', '727'],'UNIT':['%','%','%','%','%']},\n",
    "{'CHAREVENT':'O2_SAT', 'DESCRIPTION':'','ITEMID':['834', '8498', '220227', '646', '220277'],'UNIT':['%','%','%','%','%']},\n",
    "{'CHAREVENT':'RESP_RATE', 'DESCRIPTION':'','ITEMID':['618', '220210', '3603', '224689', '614', '651', '224422', '615', '224690', '619', '224688'],'UNIT':['insp/min','']},\n",
    "{'CHAREVENT':'BODY_TEMP', 'DESCRIPTION':'','ITEMID':['3655','677','676','223762','3654','678','223761','679','8537','645','591','226329','597','227054','228242'],'UNIT':['C','C','C','C','F','F','F','F','C','','','','C','','F']},\n",
    "{'CHAREVENT':'PH', 'DESCRIPTION':'','ITEMID':['3839','1673','780','1126','223830','4753','4202','860','220274','8387','1880','3777','227586','8385','1352','4755','7966'],'UNIT':['NONE','NONE','NONE','NONE','NONE','NONE','NONE','NONE','NONE','NONE','NONE','NONE','NONE','NONE','NONE','NONE','NONE']},\n",
    "{'CHAREVENT':'WEIGHT', 'DESCRIPTION':'','ITEMID':['763','224639','226512','3580','3693','3581','226531','3582'],'UNIT':['?','kg','kg','kg','kg','lb','lb','oz']},\n",
    "{'CHAREVENT':'HEIGHT', 'DESCRIPTION':'','ITEMID':['226707', '226730', '1394'],'UNIT':['?','cm','in']},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv\n",
    "# keyword = 'height'\n",
    "# event = 'chartevents'\n",
    "# samples = []\n",
    "# with open('./resources/itemid_to_variable_map.csv', 'r') as f:\n",
    "#     # r = reader(f)\n",
    "#     for line in f:\n",
    "        \n",
    "#         l = line.lower()\n",
    "#         if keyword in l and event in l:\n",
    "#             # print(l)\n",
    "#             samples.append(l)\n",
    "\n",
    "\n",
    "# print(len(samples))\n",
    "# print(samples[0])\n",
    "\n",
    "# itemid = []\n",
    "# for s in samples:\n",
    "#     s = s.split(',')\n",
    "#     itemid.append(s[5])\n",
    "\n",
    "# itemid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = pd.DataFrame(event_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHARTEVENTS_FILENAME = 'mimic-iii/CHARTEVENTS.csv'\n",
    "# READMISSION_FILENAME = 'data/readmission.csv'\n",
    "# SAMPLES_DIR = 'data/samples/'\n",
    "# ROWS_TO_READ = 1000000\n",
    "# MAX_ROWS_CHARTEVENTS = 330712483\n",
    "\n",
    "# skip_rows = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find events that relate to SUBJECT_ID, HADM_ID, ICUSTAY_ID\n",
    "    # Store said event\n",
    "    # remove said events from search pool\n",
    "\n",
    "# with open(CHARTEVENTS_FILENAME, 'r') as f:\n",
    "#     for line in f:\n",
    "#         print(line)\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chartevents = pd.read_csv(CHARTEVENTS_FILENAME,skiprows=skip_rows,nrows=ROWS_TO_READ,names=chartevents_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chartevents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chartevents_tuple = chartevents.itertuples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for e in chartevents_tuple:\n",
    "#     print(e)\n",
    "#     test = e\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading csv\n",
    "\n",
    "# explicit dtypes and/or formatting, expecially datetime\n",
    "# use HDFStore for storing data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create storage object with filename `processed_data`\n",
    "# data_store = pd.HDFStore('processed_data.h5')\n",
    "\n",
    "# # Put DataFrame into the object setting the key as 'preprocessed_df'\n",
    "# data_store['preprocessed_df'] = df\n",
    "# data_store.close()\n",
    "\n",
    "# # Access data store\n",
    "# data_store = pd.HDFStore('processed_data.h5')\n",
    "\n",
    "# # Retrieve data using key\n",
    "# preprocessed_df = data_store['preprocessed_df']\n",
    "# data_store.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import traceback\n",
    "import os\n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHARTEVENTS_FILENAME = 'mimic-iii/CHARTEVENTS.csv'\n",
    "READMISSION_FILENAME = 'data/readmission.csv'\n",
    "SAMPLES_DIR = 'data/samples/'\n",
    "DATASTORE_FILENAME = 'samples.h5'\n",
    "FEATHER_EXT = '.feather'\n",
    "ROWS_TO_READ = 1000000\n",
    "MAX_ROWS_CHARTEVENTS = 330712483\n",
    "\n",
    "skip_rows = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "chartevents_columns = [\"ROW_ID\",\"SUBJECT_ID\",\"HADM_ID\",\"ICUSTAY_ID\",\"ITEMID\",\"CHARTTIME\",\"STORETIME\",\"CGID\",\"VALUE\",\"VALUENUM\",\"VALUEUOM\",\"WARNING\",\"ERROR\",\"RESULTSTATUS\",\"STOPPED\"]\n",
    "\n",
    "\n",
    "\n",
    "# chartevents_dtype = {'ROW_ID':object,\n",
    "# 'SUBJECT_ID':np.int64,\n",
    "# 'HADM_ID':np.int64,\n",
    "# 'ICUSTAY_ID':np.int64,\n",
    "# 'ITEMID':np.int64,\n",
    "# 'CHARTTIME':object,\n",
    "# 'STORETIME':object,\n",
    "# 'CGID':np.int64,\n",
    "# 'VALUE':object,\n",
    "# 'VALUENUM':np.int64,\n",
    "# 'VALUEUOM':object,\n",
    "# 'WARNING':np.int64,\n",
    "# 'ERROR':np.int64,\n",
    "# 'RESULTSTATUS':object,\n",
    "# 'STOPPED':object}\n",
    "\n",
    "chartevents_dtype = {'ROW_ID':object,\n",
    "'SUBJECT_ID':object,\n",
    "'HADM_ID':object,\n",
    "'ICUSTAY_ID':object,\n",
    "'ITEMID':object,\n",
    "'CHARTTIME':object,\n",
    "'STORETIME':object,\n",
    "'CGID':object,\n",
    "'VALUE':object,\n",
    "'VALUENUM':object,\n",
    "'VALUEUOM':object,\n",
    "'WARNING':object,\n",
    "'ERROR':object,\n",
    "'RESULTSTATUS':object,\n",
    "'STOPPED':object}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "readmission = pd.read_csv(READMISSION_FILENAME).sort_values(['SUBJECT_ID','HADM_ID','ICUSTAY_ID']).reset_index(drop=True)\n",
    "\n",
    "# d = readmission.dtypes.to_dict()\n",
    "# d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48882, 34)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Unnamed: 0              48882\n",
       "SUBJECT_ID              35431\n",
       "HADM_ID                 45731\n",
       "ICUSTAY_ID              48882\n",
       "ADMITTIME               45470\n",
       "DISCHTIME               45519\n",
       "DEATHTIME                1778\n",
       "INTIME                  48881\n",
       "OUTTIME                 48876\n",
       "DOB                     25710\n",
       "DOD                     10717\n",
       "EXPIRE_FLAG                 2\n",
       "HOSPITAL_EXPIRE_FLAG        2\n",
       "HAS_CHARTEVENTS_DATA        2\n",
       "FIRST_CAREUNIT              5\n",
       "LAST_CAREUNIT               5\n",
       "FIRST_WARDID               14\n",
       "LAST_WARDID                14\n",
       "LOS                     32231\n",
       "INSURANCE                   5\n",
       "LANGUAGE                   74\n",
       "RELIGION                   20\n",
       "MARITAL_STATUS              7\n",
       "ETHNICITY                  41\n",
       "DIAGNOSIS               14183\n",
       "GENDER                      2\n",
       "ICU_VISIT_PER_ADMIT         7\n",
       "TRANSFERRED_RETURNED        2\n",
       "TRANSFERRED_DEATH           2\n",
       "DISCHARGED_RETURNED         2\n",
       "DISCHARGED_DEATH            2\n",
       "AGE                        84\n",
       "DIED_IN_ICU                 1\n",
       "READMISSION                 2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(readmission.shape)\n",
    "readmission.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_int(value):\n",
    "    try:\n",
    "        int(value)\n",
    "        return np.NaN\n",
    "    except ValueError:\n",
    "        return value\n",
    "    \n",
    "# a = chartevents['SUBJECT_ID'].apply(check_int).dropna()\n",
    "# #print(chartevents['SUBJECT_ID'].dtype)\n",
    "# print(a.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 200001, 200003, 200006, 200007]\n"
     ]
    }
   ],
   "source": [
    "icustay_id = list(readmission['ICUSTAY_ID'].values)\n",
    "icustay_id.insert(0,0)\n",
    "icustay_id.sort()\n",
    "print(icustay_id[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#chartevents = dd.read_csv(CHARTEVENTS_FILENAME,names=chartevents_columns,dtype=chartevents_dtype)\n",
    "chartevents = dd.read_csv(CHARTEVENTS_FILENAME,dtype=chartevents_dtype)\n",
    "chartevents = chartevents.dropna(subset=['SUBJECT_ID','HADM_ID','ICUSTAY_ID','CHARTTIME']).drop('ROW_ID',axis=1)\n",
    "#print(chartevents.head())\n",
    "try:\n",
    "    chartevents['SUBJECT_ID'] = chartevents['SUBJECT_ID'].astype(np.int64)\n",
    "    chartevents['HADM_ID'] = chartevents['HADM_ID'].astype(np.int64)\n",
    "    chartevents['ICUSTAY_ID'] = chartevents['ICUSTAY_ID'].astype(np.int64)\n",
    "    chartevents['CHARTTIME'] = dd.to_datetime(chartevents['CHARTTIME'], errors='coerce')\n",
    "    #print(chartevents.dtypes)\n",
    "except Exception as e:\n",
    "    #print('\\nError converting IDs to int in range', lines, 'to', lines + ROWS_TO_READ)\n",
    "    print(str(e))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid_chartevents = dd.multi.merge(readmission[['SUBJECT_ID','HADM_ID','ICUSTAY_ID']],chartevents,'inner',['SUBJECT_ID','HADM_ID','ICUSTAY_ID']).set_index('ICUSTAY_ID',sorted=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m row \u001b[39min\u001b[39;00m readmission\u001b[39m.\u001b[39mitertuples():\n\u001b[1;32m      2\u001b[0m     \u001b[39m#print(row.HAS_CHARTEVENTS_DATA)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     \u001b[39m#print(chartevents['SUBJECT_ID'] == row.SUBJECT_ID)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     \u001b[39m#print(np.sum(chartevents['SUBJECT_ID'] == row.SUBJECT_ID).compute())\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     df \u001b[39m=\u001b[39m chartevents[(chartevents[\u001b[39m'\u001b[39m\u001b[39mSUBJECT_ID\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m row\u001b[39m.\u001b[39mSUBJECT_ID) \u001b[39m&\u001b[39m (chartevents[\u001b[39m'\u001b[39m\u001b[39mHADM_ID\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m row\u001b[39m.\u001b[39mHADM_ID) \u001b[39m&\u001b[39m (chartevents[\u001b[39m'\u001b[39m\u001b[39mICUSTAY_ID\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m row\u001b[39m.\u001b[39mICUSTAY_ID)]\n\u001b[0;32m----> 6\u001b[0m     dd\u001b[39m.\u001b[39;49mto_parquet(df,SAMPLES_DIR,append\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m      7\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/media/Bencsik/SSD1/CS 598 Project/Code/.venv/lib/python3.10/site-packages/dask/dataframe/io/parquet/core.py:1069\u001b[0m, in \u001b[0;36mto_parquet\u001b[0;34m(df, path, engine, compression, write_index, append, overwrite, ignore_divisions, partition_on, storage_options, custom_metadata, write_metadata_file, compute, compute_kwargs, schema, name_function, filesystem, **kwargs)\u001b[0m\n\u001b[1;32m   1066\u001b[0m out \u001b[39m=\u001b[39m Scalar(graph, final_name, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1068\u001b[0m \u001b[39mif\u001b[39;00m compute:\n\u001b[0;32m-> 1069\u001b[0m     out \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39;49mcompute(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcompute_kwargs)\n\u001b[1;32m   1071\u001b[0m \u001b[39m# Invalidate the filesystem listing cache for the output path after write.\u001b[39;00m\n\u001b[1;32m   1072\u001b[0m \u001b[39m# We do this before returning, even if `compute=False`. This helps ensure\u001b[39;00m\n\u001b[1;32m   1073\u001b[0m \u001b[39m# that reading files that were just written succeeds.\u001b[39;00m\n\u001b[1;32m   1074\u001b[0m fs\u001b[39m.\u001b[39minvalidate_cache(path)\n",
      "File \u001b[0;32m/media/Bencsik/SSD1/CS 598 Project/Code/.venv/lib/python3.10/site-packages/dask/base.py:314\u001b[0m, in \u001b[0;36mDaskMethodsMixin.compute\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    291\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \n\u001b[1;32m    293\u001b[0m \u001b[39m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[39m    dask.base.compute\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 314\u001b[0m     (result,) \u001b[39m=\u001b[39m compute(\u001b[39mself\u001b[39;49m, traverse\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    315\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/media/Bencsik/SSD1/CS 598 Project/Code/.venv/lib/python3.10/site-packages/dask/base.py:599\u001b[0m, in \u001b[0;36mcompute\u001b[0;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[1;32m    596\u001b[0m     keys\u001b[39m.\u001b[39mappend(x\u001b[39m.\u001b[39m__dask_keys__())\n\u001b[1;32m    597\u001b[0m     postcomputes\u001b[39m.\u001b[39mappend(x\u001b[39m.\u001b[39m__dask_postcompute__())\n\u001b[0;32m--> 599\u001b[0m results \u001b[39m=\u001b[39m schedule(dsk, keys, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    600\u001b[0m \u001b[39mreturn\u001b[39;00m repack([f(r, \u001b[39m*\u001b[39ma) \u001b[39mfor\u001b[39;00m r, (f, a) \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[0;32m/media/Bencsik/SSD1/CS 598 Project/Code/.venv/lib/python3.10/site-packages/dask/threaded.py:89\u001b[0m, in \u001b[0;36mget\u001b[0;34m(dsk, keys, cache, num_workers, pool, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(pool, multiprocessing\u001b[39m.\u001b[39mpool\u001b[39m.\u001b[39mPool):\n\u001b[1;32m     87\u001b[0m         pool \u001b[39m=\u001b[39m MultiprocessingPoolExecutor(pool)\n\u001b[0;32m---> 89\u001b[0m results \u001b[39m=\u001b[39m get_async(\n\u001b[1;32m     90\u001b[0m     pool\u001b[39m.\u001b[39;49msubmit,\n\u001b[1;32m     91\u001b[0m     pool\u001b[39m.\u001b[39;49m_max_workers,\n\u001b[1;32m     92\u001b[0m     dsk,\n\u001b[1;32m     93\u001b[0m     keys,\n\u001b[1;32m     94\u001b[0m     cache\u001b[39m=\u001b[39;49mcache,\n\u001b[1;32m     95\u001b[0m     get_id\u001b[39m=\u001b[39;49m_thread_get_id,\n\u001b[1;32m     96\u001b[0m     pack_exception\u001b[39m=\u001b[39;49mpack_exception,\n\u001b[1;32m     97\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m     98\u001b[0m )\n\u001b[1;32m    100\u001b[0m \u001b[39m# Cleanup pools associated to dead threads\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[39mwith\u001b[39;00m pools_lock:\n",
      "File \u001b[0;32m/media/Bencsik/SSD1/CS 598 Project/Code/.venv/lib/python3.10/site-packages/dask/local.py:500\u001b[0m, in \u001b[0;36mget_async\u001b[0;34m(submit, num_workers, dsk, result, cache, get_id, rerun_exceptions_locally, pack_exception, raise_exception, callbacks, dumps, loads, chunksize, **kwargs)\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[39mwhile\u001b[39;00m state[\u001b[39m\"\u001b[39m\u001b[39mwaiting\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mor\u001b[39;00m state[\u001b[39m\"\u001b[39m\u001b[39mready\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mor\u001b[39;00m state[\u001b[39m\"\u001b[39m\u001b[39mrunning\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m    499\u001b[0m     fire_tasks(chunksize)\n\u001b[0;32m--> 500\u001b[0m     \u001b[39mfor\u001b[39;00m key, res_info, failed \u001b[39min\u001b[39;00m queue_get(queue)\u001b[39m.\u001b[39mresult():\n\u001b[1;32m    501\u001b[0m         \u001b[39mif\u001b[39;00m failed:\n\u001b[1;32m    502\u001b[0m             exc, tb \u001b[39m=\u001b[39m loads(res_info)\n",
      "File \u001b[0;32m/media/Bencsik/SSD1/CS 598 Project/Code/.venv/lib/python3.10/site-packages/dask/local.py:137\u001b[0m, in \u001b[0;36mqueue_get\u001b[0;34m(q)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mqueue_get\u001b[39m(q):\n\u001b[0;32m--> 137\u001b[0m     \u001b[39mreturn\u001b[39;00m q\u001b[39m.\u001b[39;49mget()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/queue.py:171\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[39melif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    170\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_qsize():\n\u001b[0;32m--> 171\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnot_empty\u001b[39m.\u001b[39;49mwait()\n\u001b[1;32m    172\u001b[0m \u001b[39melif\u001b[39;00m timeout \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    173\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m'\u001b[39m\u001b[39m must be a non-negative number\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    321\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for row in readmission.itertuples():\n",
    "    #print(row.HAS_CHARTEVENTS_DATA)\n",
    "    #print(chartevents['SUBJECT_ID'] == row.SUBJECT_ID)\n",
    "    #print(np.sum(chartevents['SUBJECT_ID'] == row.SUBJECT_ID).compute())\n",
    "    df = chartevents[(chartevents['SUBJECT_ID'] == row.SUBJECT_ID) & (chartevents['HADM_ID'] == row.HADM_ID) & (chartevents['ICUSTAY_ID'] == row.ICUSTAY_ID)]\n",
    "    dd.to_parquet(df,SAMPLES_DIR,append=True)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "#print(chartevents.shape)\n",
    "#chartevents = chartevents.sort_values(['SUBJECT_ID','HADM_ID','ICUSTAY_ID']).reset_index(drop=True)\n",
    "\n",
    "\n",
    "#print(valid_chartevents.npartitions)\n",
    "\n",
    "# max_chartevents_charttime = valid_chartevents.map_partitions(lambda df: df.groupby('ICUSTAY_ID')['CHARTTIME'].max().reset_index(name='MAX_CHARTTIME'))\n",
    "# max_chartevents_charttime = max_chartevents_charttime.map_partitions(lambda df : df.sort_values(['ICUSTAY_ID']).reset_index(drop=True))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#valid_chartevents['MAX_CHARTTIME'] = valid_chartevents['CHARTTIME'].max()\n",
    "#print(valid_chartevents.get_partition(1).head(1))\n",
    "\n",
    "#valid_chartevents = valid_chartevents.groupby(['ICUSTAY_ID'],sort=False,split_out=valid_chartevents.npartitions)\n",
    "\n",
    "\n",
    "\n",
    "#valid_chartevents = valid_chartevents.sort_values(['ICUSTAY_ID']).reset_index(drop=True)\n",
    "\n",
    "# valid_chartevents = valid_chartevents.repartition(divisions=icustay_id)\n",
    "# print(valid_chartevents.npartitions)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#print(valid_chartevents.shape)\n",
    "#valid_chartevents = valid_chartevents.sort_values(['SUBJECT_ID','HADM_ID','ICUSTAY_ID','CHARTTIME']).reset_index(drop=True)\n",
    "#print(valid_chartevents.divisions)\n",
    "\n",
    "# valid_chartevents = valid_chartevents.groupby(['SUBJECT_ID','HADM_ID','ICUSTAY_ID'])\n",
    "\n",
    "\n",
    "# print(len(valid_chartevents.groupby(['SUBJECT_ID','HADM_ID','ICUSTAY_ID'])['SUBJECT_ID','HADM_ID','ICUSTAY_ID'].unique().compute()))\n",
    "\n",
    "# v = valid_chartevents.groupby(['SUBJECT_ID','HADM_ID','ICUSTAY_ID'])\n",
    "# print(v['ICUSTAY_ID'].unique().compute())\n",
    "# for name, group in v:\n",
    "#     group.to_parquet(SAMPLES_DIR)  \n",
    "    # print(name)\n",
    "    # print(group.head(1),end='\\n\\n')\n",
    "    # group = group.sort_values(['SUBJECT_ID','HADM_ID','ICUSTAY_ID']).reset_index(drop=True)\n",
    "\n",
    "    # name = str(name).strip('() ').replace(' ','').split(',')\n",
    "    # filename = 'sample_' + '_'.join(name) + FEATHER_EXT\n",
    "    \n",
    "\n",
    "    # try:\n",
    "    #     group.to_feather(SAMPLES_DIR + filename)\n",
    "    # except Exception as e:\n",
    "    #     pass\n",
    "    \n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    SUBJECT_ID  HADM_ID  ICUSTAY_ID  ITEMID           CHARTTIME   \n",
      "0         7095   191813      294833  220180 2201-07-17 10:00:00  \\\n",
      "1         7095   191813      294833  220181 2201-07-17 10:00:00   \n",
      "2         7095   191813      294833  220210 2201-07-17 10:00:00   \n",
      "3         7095   191813      294833  220045 2201-07-17 11:00:00   \n",
      "4         7095   191813      294833  220050 2201-07-17 11:00:00   \n",
      "..         ...      ...         ...     ...                 ...   \n",
      "95        7101   161140      297446  223761 2162-04-04 00:00:00   \n",
      "96        7101   161140      297446  224639 2162-04-04 00:00:00   \n",
      "97        7101   161140      297446  224562 2162-04-04 00:11:00   \n",
      "98        7101   161140      297446  224846 2162-04-04 00:11:00   \n",
      "99        7101   161140      297446  220045 2162-04-04 01:00:00   \n",
      "\n",
      "              STORETIME   CGID VALUE VALUENUM  VALUEUOM WARNING ERROR   \n",
      "0   2201-07-17 09:49:00  15652    44       44      mmHg       0     0  \\\n",
      "1   2201-07-17 09:49:00  15652    52       52      mmHg       0     0   \n",
      "2   2201-07-17 09:50:00  15652    17       17  insp/min       0     0   \n",
      "3   2201-07-17 11:40:00  15652    77       77       bpm       0     0   \n",
      "4   2201-07-17 11:40:00  15652    94       94      mmHg       0     0   \n",
      "..                  ...    ...   ...      ...       ...     ...   ...   \n",
      "95  2162-04-04 00:03:00  18820  99.6     99.6        ?F       0     0   \n",
      "96  2162-04-04 00:05:00  18820  66.8     66.8        kg       0     0   \n",
      "97  2162-04-04 00:11:00  18820     1        1        cm       0     0   \n",
      "98  2162-04-04 00:11:00  18820    .5       .5        cm       0     0   \n",
      "99  2162-04-04 02:18:00  18820    84       84       bpm       0     0   \n",
      "\n",
      "   RESULTSTATUS STOPPED  \n",
      "0           NaN     NaN  \n",
      "1           NaN     NaN  \n",
      "2           NaN     NaN  \n",
      "3           NaN     NaN  \n",
      "4           NaN     NaN  \n",
      "..          ...     ...  \n",
      "95          NaN     NaN  \n",
      "96          NaN     NaN  \n",
      "97          NaN     NaN  \n",
      "98          NaN     NaN  \n",
      "99          NaN     NaN  \n",
      "\n",
      "[100 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "print(valid_chartevents.get_partition(1).head(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "551"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_chartevents.npartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_chartevents.divisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Works, but `pc.read_csv()` results in increasing time to process, hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #samples = os.listdir(SAMPLES_DIR)\n",
    "# #while skip_rows < MAX_ROWS_CHARTEVENTS:\n",
    "# sample_set = set()\n",
    "\n",
    "# max_range = int(MAX_ROWS_CHARTEVENTS / ROWS_TO_READ) + 1\n",
    "# for lines in trange(max_range):\n",
    "    \n",
    "#     chartevents = pd.read_csv(CHARTEVENTS_FILENAME,skiprows=lines*ROWS_TO_READ+1,nrows=ROWS_TO_READ,names=chartevents_columns,dtype=chartevents_dtype)\n",
    "    \n",
    "#     #skip_rows += ROWS_TO_READ\n",
    "    \n",
    "\n",
    "#     # need all 3 IDs to create data set. Drop nulls\n",
    "#     # ICUSTAY_ID is most unique\n",
    "\n",
    "#     #chartevents = chartevents[chartevents['SUBJECT_ID'].notnull() & chartevents['HADM_ID'].notnull() & chartevents['ICUSTAY_ID'].notnull()]\n",
    "#     chartevents = chartevents.dropna(subset=['SUBJECT_ID','HADM_ID','ICUSTAY_ID']).drop('ROW_ID',axis=1)\n",
    "    \n",
    "\n",
    "#     try:\n",
    "#         chartevents['SUBJECT_ID'] = chartevents['SUBJECT_ID'].astype(np.int64,errors='ignore')\n",
    "#         chartevents['HADM_ID'] = chartevents['HADM_ID'].astype(np.int64)\n",
    "#         chartevents['ICUSTAY_ID'] = chartevents['ICUSTAY_ID'].astype(np.int64)\n",
    "#         #print(chartevents.dtypes)\n",
    "#     except Exception as e:\n",
    "#         print('\\nError converting IDs to int in range', lines, 'to', lines + ROWS_TO_READ)\n",
    "#         print(str(e))\n",
    "#         #print(traceback.format_exc())\n",
    "#         continue\n",
    "#         # need more clean up to continue\n",
    "#         # need to show more logging data, such as which chunk of chartevents had issue\n",
    "#         # change while to for loop, use tqdm\n",
    "\n",
    "\n",
    "#     #print(chartevents.shape)\n",
    "#     #chartevents = chartevents.sort_values(['SUBJECT_ID','HADM_ID','ICUSTAY_ID']).reset_index(drop=True)\n",
    "\n",
    "#     valid_chartevents = pd.merge(readmission[['SUBJECT_ID','HADM_ID','ICUSTAY_ID']],chartevents,'inner',['SUBJECT_ID','HADM_ID','ICUSTAY_ID'])\n",
    "    \n",
    "\n",
    "#     for name, group in valid_chartevents.groupby(['SUBJECT_ID','HADM_ID','ICUSTAY_ID']):\n",
    "#         # print(name)\n",
    "#         # print(group.head(1),end='\\n\\n')\n",
    "#         group = group.sort_values(['SUBJECT_ID','HADM_ID','ICUSTAY_ID']).reset_index(drop=True)\n",
    "\n",
    "#         name = str(name).strip('() ').replace(' ','').split(',')\n",
    "#         filename = 'sample_' + '_'.join(name) + FEATHER_EXT\n",
    "        \n",
    "#         # if filename in sample_set:\n",
    "#         #     read_df = pd.read_feather(SAMPLES_DIR + filename)\n",
    "#         #     group = pd.concat([read_df,group]).reset_index(drop=True)\n",
    "#         #     os.remove(SAMPLES_DIR + filename)\n",
    "#         # else:\n",
    "#         #     sample_set.add(filename)\n",
    "\n",
    "#         # group.to_hdf(''.join([SAMPLES_DIR,DATASTORE_FILENAME]),'sample' + '_'.join(name),'a',append=True, index=False)\n",
    "#         try:\n",
    "#             group.to_feather(SAMPLES_DIR + filename)\n",
    "#         except Exception as e:\n",
    "#             print('\\nFeather write error. Iteration: ', lines, '\\tGroup:', name)\n",
    "#             continue\n",
    "    \n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #samples = os.listdir(SAMPLES_DIR)\n",
    "# while skip_rows < MAX_ROWS_CHARTEVENTS:\n",
    "#     chartevents = pd.read_csv(CHARTEVENTS_FILENAME,skiprows=skip_rows,nrows=ROWS_TO_READ,names=chartevents_columns,dtype=chartevents_dtype)\n",
    "#     # print(chartevents.dtypes)\n",
    "#     skip_rows += ROWS_TO_READ\n",
    "\n",
    "#     while len(chartevents) > 0:\n",
    "#         # get first row\n",
    "#         # t_row = chartevents.iloc[0]\n",
    "\n",
    "#         chartevents = chartevents.sort_values(['SUBJECT_ID','HADM_ID','ICUSTAY_ID']).reset_index(drop=True)\n",
    "\n",
    "#         try:\n",
    "#             subject = chartevents.iloc[0]['SUBJECT_ID']\n",
    "#             #print('subject', type(subject))\n",
    "#             hadm_id = chartevents.iloc[0]['HADM_ID']\n",
    "#             icustay_id = chartevents.iloc[0]['ICUSTAY_ID']\n",
    "#         except:\n",
    "#             chartevents = chartevents.drop(0)\n",
    "#             continue\n",
    "\n",
    "#         # in the readmission dataframe?\n",
    "#         #print('chart', subject, hadm_id, icustay_id)\n",
    "#         #print('readmission', readmission['SUBJECT_ID'], readmission['HADM_ID'], readmission['ICUSTAY_ID'])\n",
    "\n",
    "#         for row in readmission.itertuples():\n",
    "#             if ((row.SUBJECT_ID == subject) and (row.HADM_ID == hadm_id) and (row.ICUSTAY_ID == icustay_id)):\n",
    "#                 temp = chartevents[(chartevents['SUBJECT_ID'] == subject) & (chartevents['HADM_ID'] == hadm_id) & (chartevents['ICUSTAY_ID'] == icustay_id)]\n",
    "#                 temp.to_hdf(''.join([SAMPLES_DIR,DATASTORE_FILENAME]),'_'.join(['sample',str(subject),str(hadm_id),str(icustay_id)]),'a',append=True)\n",
    "            \n",
    "        \n",
    "\n",
    "        \n",
    "#         chartevents = chartevents[(chartevents['SUBJECT_ID'] != subject) & (chartevents['HADM_ID'] != hadm_id) & (chartevents['ICUSTAY_ID'] != icustay_id)]\n",
    "#         print((ROWS_TO_READ - len(chartevents))/ROWS_TO_READ)\n",
    "#         print(chartevents.shape)\n",
    "    \n",
    "#     print(skip_rows/MAX_ROWS_CHARTEVENTS)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['5', '6', '8'] <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "a = tuple((5,6,8))\n",
    "s = str(a).strip('() ').replace(' ','').split(',')\n",
    "print(s, type(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
